import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from google.colab import files

uploaded = files.upload()
file_path = list(uploaded.keys())[0]

df = pd.read_csv(file_path, sep='\t')
df['Income'].fillna(df['Income'].median(), inplace=True)
df['Age'] = 2025 - df['Year_Birth']
df['Dt_Customer'] = pd.to_datetime(df['Dt_Customer'], dayfirst=True)
df['Customer_Lifetime'] = (pd.to_datetime('2025-01-01') - df['Dt_Customer']).dt.days
df['Dependents'] = df['Kidhome'] + df['Teenhome']
drop_cols = ['ID', 'Year_Birth', 'Dt_Customer', 'Z_CostContact', 'Z_Revenue', 'Kidhome', 'Teenhome']
df.drop(columns=drop_cols, inplace=True)
df = pd.get_dummies(df, columns=['Education', 'Marital_Status'], drop_first=True, dtype=int)

plt.figure(figsize=(12, 10))
sns.heatmap(df.corr(), cmap='coolwarm', linewidths=0.5)
plt.title("Feature Correlation Heatmap")
plt.show()

numeric_cols = ['Income', 'Age', 'Recency', 'MntWines', 'MntFruits']
df[numeric_cols].hist(figsize=(12, 8), bins=20)
plt.tight_layout()
plt.show()

scaler = StandardScaler()
scaled_data = scaler.fit_transform(df)
pca = PCA(n_components=2, random_state=42)
reduced_data = pca.fit_transform(scaled_data)

sil_scores = []
k_range = range(2, 7)
for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = kmeans.fit_predict(reduced_data)
    score = silhouette_score(reduced_data, labels)
    sil_scores.append(score)
optimal_k = k_range[np.argmax(sil_scores)]

kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
df['Cluster'] = kmeans.fit_predict(reduced_data)

plt.figure(figsize=(10, 7))
sns.scatterplot(x=reduced_data[:, 0], y=reduced_data[:, 1], hue=df['Cluster'], palette='viridis', s=60)
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='X', s=200, label='Centroids')
plt.title("KMeans Clustering on PCA-reduced Data")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.legend()
plt.show()




import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.metrics import silhouette_score
from scipy.stats import ttest_ind
from google.colab import files

uploaded = files.upload()
file_path = list(uploaded.keys())[0]

df = pd.read_csv(file_path, sep='\t')
df['Income'].fillna(df['Income'].median(), inplace=True)
df['Age'] = 2025 - df['Year_Birth']
df['Dt_Customer'] = pd.to_datetime(df['Dt_Customer'], dayfirst=True)
df['Customer_Lifetime'] = (pd.to_datetime('2025-01-01') - df['Dt_Customer']).dt.days
df['Dependents'] = df['Kidhome'] + df['Teenhome']
drop_cols = ['ID', 'Year_Birth', 'Dt_Customer', 'Z_CostContact', 'Z_Revenue', 'Kidhome', 'Teenhome']
df.drop(columns=drop_cols, inplace=True)
df = pd.get_dummies(df, columns=['Education', 'Marital_Status'], drop_first=True, dtype=int)

print("Descriptive Statistics:")
print(df.describe())

scaler = StandardScaler()
scaled_data = scaler.fit_transform(df)
pca = PCA(n_components=2, random_state=42)
reduced_data = pca.fit_transform(scaled_data)

print("\n" + "="*50)
print("Silhouette Scores for Different Values of k")
print("="*50)
sil_scores = []
k_range = range(2, 7)
for k in k_range:
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels = kmeans.fit_predict(reduced_data)
    score = silhouette_score(reduced_data, labels)
    sil_scores.append(score)
    print(f"Silhouette Score for k={k}: {score:.3f}")

optimal_k = k_range[np.argmax(sil_scores)]
print(f"\nOptimal number of clusters: {optimal_k}")

kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
df['Cluster'] = kmeans.fit_predict(reduced_data)
final_silhouette = silhouette_score(reduced_data, df['Cluster'])
print(f"\nFinal Silhouette Score for k={optimal_k}: {final_silhouette:.3f}")

print("\n" + "="*50)
print("Cluster Summary (Mean and Count per Feature)")
print("="*50)
cluster_summary = df.groupby('Cluster').agg(['mean', 'count'])
print(cluster_summary)

print("\n" + "="*50)
print("Hypothesis Testing: T-test for Differences Between Clusters")
print("="*50)
numeric_features = ['Income', 'Age', 'Recency', 'MntWines']
for feature in numeric_features:
    clusters = df['Cluster'].unique()
    if len(clusters) == 2:
        group0 = df[df['Cluster'] == clusters[0]][feature]
        group1 = df[df['Cluster'] == clusters[1]][feature]
        t_stat, p_val = ttest_ind(group0, group1)
        print(f"Feature '{feature}': t-statistic = {t_stat:.2f}, p-value = {p_val:.3f}")

print("\n" + "="*50)
print("Comparative Model Analysis")
print("="*50)
hier_model = AgglomerativeClustering(n_clusters=optimal_k)
hier_labels = hier_model.fit_predict(reduced_data)
hier_score = silhouette_score(reduced_data, hier_labels)
print(f"Hierarchical Clustering Silhouette Score: {hier_score:.3f}")
print(f"KMeans Clustering Silhouette Score:       {final_silhouette:.3f}")
